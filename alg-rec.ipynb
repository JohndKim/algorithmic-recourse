{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Depthm', 'T_degC', 'Salnty', 'O2ml_L', 'STheta', 'O2Sat']\n",
      "['Depthm', 'T_degC', 'O2ml_L', 'STheta', 'O2Sat']\n",
      "691628    14.99\n",
      "828063     7.22\n",
      "753076     9.39\n",
      "530014     9.54\n",
      "300976     5.64\n",
      "          ...  \n",
      "390087    13.80\n",
      "813764     6.49\n",
      "522940    11.57\n",
      "757369    15.67\n",
      "827470    12.46\n",
      "Name: T_degC, Length: 132254, dtype: float64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from iron import phi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# take 80% train, use random forest for model, then select a case from test set, then run\n",
    "predictor = \"T_degC\"\n",
    "\n",
    "# returns features, X: features and y: the feature we predict (predictor)\n",
    "def prepare_data():\n",
    "    ######### IMPORT DATA # ########\n",
    "    file = '/Users/johnkim/Downloads/archive (1)/bottle.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    pd.set_option('display.max_columns', None) # view all cols\n",
    "\n",
    "    # REMOVE COLUMNS WITH MORE THAN 10% NA VALUES\n",
    "    min_na = len(df) * .1 # removes all cols with more than 10% NA values\n",
    "    df = df.dropna(thresh=min_na, axis=1)\n",
    "    df = df.drop(['Sta_ID', 'Depth_ID', 'Cst_Cnt', 'Btl_Cnt', 'BtlNum', 'RecInd','T_prec'], axis=1)\n",
    "    # df.head(10)\n",
    "    # print(df.head(10))\n",
    "\n",
    "    # LIMITS FEATURES TO ONLY 10\n",
    "    features = list(df.columns.values) # get features // limited columns\n",
    "    # print(features)\n",
    "    features = features[0:6]\n",
    "    new_df = df[features].copy().dropna() # same num rows\n",
    "    print(features)\n",
    "\n",
    "    # remove feature we want to predict\n",
    "    features.pop(2)\n",
    "    print(features)\n",
    "    # features = features[0, 1:6]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # FEATURES\n",
    "    X = new_df.loc[:, features] \n",
    "    # PREDICTOR\n",
    "    y = new_df.loc[:, [predictor]]\n",
    "    \n",
    "    return features, X, y\n",
    "\n",
    "def ttsplit(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=.80)\n",
    "    # reg = RandomForestRegressor(max_depth = 2, random_state = 0)\n",
    "    # reg.fit(X_train, y_train)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# returns phi relevance function and the relevance numpy array of the predictor (y)\n",
    "# CHECK WITH NUNO\n",
    "def get_phis(y_train, y_test):\n",
    "    ph = phi.phi_control(y_train[predictor], extr_type=\"both\")\n",
    "    phis = phi.phi(y_test[predictor], phi_parms=ph)\n",
    "    # print(phis)\n",
    "    print(y_test[predictor])\n",
    "    print(type(y_test[predictor]))\n",
    "    # phis[1]\n",
    "    return ph, phis\n",
    "\n",
    "base = [] # original feature values\n",
    "pbounds = {}\n",
    "\n",
    "# returns regression model, y original, and y relevance\n",
    "def create_regressor(features, X_train, X_test, y_train, y_test, phis):\n",
    "    reg = RandomForestRegressor(n_estimators=500, random_state = 0) # n = 500\n",
    "    # reg = LinearRegression().fit(X_train, y_train)\n",
    "    # svm regression or multilayer small neural network (multilayer perceptron)\n",
    "    # random forest\n",
    "    # try mars \n",
    "    # try lightgbm \n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # NOT NECESSARY\n",
    "    \n",
    "    \n",
    "    i = random.randint(0, len(X_test)) # random variable\n",
    "    \n",
    "    # actual y value\n",
    "    y_orig = y_test.iloc[i][predictor]\n",
    "    # y relevance value\n",
    "    y_rel = phis[i]\n",
    "    \n",
    "    # creating pbounds based on mean of features\n",
    "    for f in features: \n",
    "        x = X_test.iloc[i][f]\n",
    "        base.append(x)\n",
    "        sd = 0.05 # calculates sd \n",
    "        pbounds[f] = (x - (sd * x), x + (sd * x))\n",
    "        \n",
    "\n",
    "    return base, pbounds, reg, y_orig, y_rel\n",
    "\n",
    "\n",
    "\n",
    "def create_vars():\n",
    "    i = random.randint(0, len(X_test)) # random variable\n",
    "\n",
    "    # actual y value\n",
    "    y_orig = y_test.iloc[i][predictor]\n",
    "    # y relevance value\n",
    "    y_rel = phis[i]\n",
    "    \n",
    "    # creating pbounds based on mean of features\n",
    "    for f in features: \n",
    "        x = X_test.iloc[i][f]\n",
    "        base.append(x)\n",
    "        sd = 0.05\n",
    "        pbounds[f] = (x - (sd * x), x + (sd * x))\n",
    "    \n",
    "    return base, pbounds, y_orig, y_rel\n",
    "\n",
    "\n",
    "features, X, y = prepare_data()\n",
    "X_train, X_test, y_train, y_test = ttsplit(X, y)\n",
    "# reg, y_orig, y_rel = create_regressor(features, X_train)\n",
    "ph, phis = get_phis(y_train, y_test)\n",
    "base, pbounds, reg, y_orig, y_rel = create_regressor(features, X_train, X_test, y_train, y_test, phis)\n",
    "\n",
    "# x_orig = 15\n",
    "# y_orig = 34\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_all_abs_y():\n",
    "    pdiffs_abs_y = []\n",
    "    params = []\n",
    "\n",
    "    for i in range(0, len(X_test), 10):\n",
    "        print(i)    \n",
    "        # actual y value\n",
    "        y_orig = y_test.iloc[i][predictor]\n",
    "        print(y_orig)\n",
    "        # y relevance value\n",
    "        # y_rel = phis[i]\n",
    "        \n",
    "        # creating pbounds based on mean of features\n",
    "        for f in features: \n",
    "            x = X_test.iloc[i][f]\n",
    "            base.append(x)\n",
    "            sd = 0.05\n",
    "            pbounds[f] = (x - (sd * x), x + (sd * x))\n",
    "            \n",
    "            \n",
    "        def black_box_function_abs_y(Depthm, T_degC, Salnty, O2ml_L, STheta, O2Sat, BtlNum, RecInd, T_prec):\n",
    "    \n",
    "            # when generating x, we will have a list of vars with all the numeric and categorical\n",
    "            \n",
    "            x = [[BtlNum, Depthm, O2Sat, O2ml_L, RecInd, STheta, Salnty, T_degC, T_prec]]\n",
    "            print(\"x:\", x)\n",
    "\n",
    "            y1 = reg.predict(x)    \n",
    "            print(\"y1:\", y1)\n",
    "            pdiff = abs(y1[0] - y_orig)\n",
    "                \n",
    "            print(pdiff)\n",
    "            return pdiff[0]\n",
    "        \n",
    "\n",
    "        ########### CREATING BAYESIAN OPTIMIZER ###########\n",
    "        optimizer_abs_y = BayesianOptimization(\n",
    "            f=black_box_function_abs_y,  # f(x)\n",
    "            pbounds=pbounds,  # parameter bounds\n",
    "            random_state=1,\n",
    "            allow_duplicate_points=True,\n",
    "        )\n",
    "\n",
    "        ########### MAXIMIZING (finding optimums) ###########\n",
    "        optimizer_abs_y.maximize(\n",
    "            init_points=5,  # num of random steps to perform --> can help diversify explore space\n",
    "            n_iter=50,  # number of iterations/steps of baye opt --> more = more accurate optimum\n",
    "        )\n",
    "                \n",
    "        print(optimizer_abs_y.max['target'], optimizer_abs_y.max['params'])\n",
    "        print(\"target\")\n",
    "        print(optimizer_abs_y.max['target'])\n",
    "        print(\"params\")\n",
    "        print(optimizer_abs_y.max['params'])\n",
    "        pdiffs_abs_y.append(optimizer_abs_y.max['target'])\n",
    "        params.append(optimizer_abs_y.max['params'])\n",
    "        \n",
    "    return pdiffs_abs_y, params\n",
    "    \n",
    "pdiffs_abs_y, params = run_all_abs_y()\n",
    "# [15.2, 80.75, 55.385, 3.7904999999999998, 2.85, 24.396, 31.985549999999996, 11.907, 1.9]\n",
    "# [15.2, 80.75, 55.38, 3.79, 2.85, 24.4, 31.99, 11.91, 1.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# see prediction of the y value of that new case\n",
    "    # then find the y rel for that new case\n",
    "# repeat in the prediction model with original and new y rel\n",
    "\n",
    "def run_all_abs_rel():\n",
    "    pdiffs_abs_rel = []\n",
    "    params_rel = []\n",
    "\n",
    "    for i in range(0, len(X_test), 10):\n",
    "        print(i)\n",
    "        \n",
    "        # actual y value\n",
    "        # y_orig = y_test.iloc[i][predictor]\n",
    "        # y relevance value\n",
    "        y_rel = phis[i]\n",
    "        print(y_test.iloc[i][predictor])\n",
    "        \n",
    "        # creating pbounds based on mean of features\n",
    "        for f in features: \n",
    "            x = X_test.iloc[i][f]\n",
    "            base.append(x)\n",
    "            sd = 0.05\n",
    "            pbounds[f] = (x - (sd * x), x + (sd * x))\n",
    "            \n",
    "            \n",
    "        def black_box_function_abs_rel(Depthm, T_degC, Salnty, O2ml_L, STheta, O2Sat, BtlNum, RecInd, T_prec):\n",
    "            # when generating x, we will have a list of vars with all the numeric and categorical\n",
    "            x = [[BtlNum, Depthm, O2Sat, O2ml_L, RecInd, STheta, Salnty, T_degC, T_prec]]\n",
    "            print(x)\n",
    "            \n",
    "            y1 = reg.predict(x)\n",
    "            print(y1[0][0])\n",
    "            \n",
    "            y_rel_pred = phi.phi(pd.Series(y1[0][0]), phi_parms=ph)\n",
    "            print(\"rel: \", y_rel_pred)\n",
    "            # print(phi.phi(pd.Series(y_test.iloc[i][predictor]), phi_parms=ph))\n",
    "            \n",
    "            # relevance - relevance of predicted y\n",
    "            pdiff = abs(y_rel - y_rel_pred[0])\n",
    "            # print(pdiff)\n",
    "            return pdiff\n",
    "        \n",
    "        \n",
    "        ########### CREATING BAYESIAN OPTIMIZER ###########\n",
    "        optimizer_abs_rel = BayesianOptimization(\n",
    "            f=black_box_function_abs_rel,  # f(x)\n",
    "            pbounds=pbounds,  # parameter bounds\n",
    "            random_state=1,\n",
    "            allow_duplicate_points=True,\n",
    "        )\n",
    "\n",
    "        ########### MAXIMIZING (finding optimums) ###########\n",
    "        optimizer_abs_rel.maximize(\n",
    "            init_points=5,  # num of random steps to perform --> can help diversify explore space\n",
    "            n_iter=25,  # number of iterations/steps of baye opt --> more = more accurate optimum\n",
    "        )\n",
    "        \n",
    "        pdiffs_abs_rel.append(optimizer_abs_rel.max['target'])\n",
    "        params_rel.append(optimizer_abs_rel.max['params'])\n",
    "        \n",
    "        # print(optimizer_abs_rel.max['target'])\n",
    "        # print(optimizer_abs_rel.max['params'])\n",
    "        \n",
    "        print(optimizer_abs_rel.max['target'], optimizer_abs_rel.max['params'])\n",
    "        print(\"target\")\n",
    "        print(optimizer_abs_rel.max['target'])\n",
    "        print(\"params\")\n",
    "        print(optimizer_abs_rel.max['params'])\n",
    "\n",
    "        \n",
    "    return pdiffs_abs_rel, params_rel\n",
    "    \n",
    "pdiffs_abs_rel, params_rel = run_all_abs_rel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY HAVE TO DO IF WE RUN THE MODEL AGAIN WITH UPDATES\n",
    "\n",
    "def convert_to_dfs():\n",
    "    df1 = pd.DataFrame(pdiffs_abs_y)\n",
    "    # df6.rename(columns={0: \"Difference\"})\n",
    "    # df1\n",
    "    df1.to_csv(\"difference_no_relevance.csv\")    \n",
    "    # df1.to_csv(\"new.csv\")\n",
    "\n",
    "\n",
    "    df2 = pd.DataFrame(pdiffs_abs_rel)\n",
    "    # df2.rename(columns={0: \"Difference\"})\n",
    "    # df2\n",
    "    df2.to_csv(\"difference_yes_relevance.csv\")\n",
    "\n",
    "    df3= pd.DataFrame(params)\n",
    "    # df3\n",
    "    df3.to_csv(\"new_f.csv\")\n",
    "\n",
    "    df3.to_csv(\"features_no_relevance.csv\")\n",
    "\n",
    "    df4 = pd.DataFrame(params_rel)\n",
    "    # df4\n",
    "    df4.to_csv(\"features_yes_relevance.csv\")\n",
    "\n",
    "convert_to_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f_abs_file = \"/Users/johnkim/Developer/features_no_relevance.csv\"\n",
    "# f_abs_file = \"/Users/johnkim/Developer/new_f.csv\"\n",
    "\n",
    "# f_rel_file = \"/Users/johnkim/Developer/features_yes_relevance.csv\"\n",
    "pdiffs_abs_file = \"/Users/johnkim/Developer/difference_no_relevance.csv\"\n",
    "pdiffs_abs_file = \"/Users/johnkim/Developer/new.csv\"\n",
    "\n",
    "pdiffs_rel_file = \"/Users/johnkim/Developer/difference_yes_relevance.csv\"\n",
    "\n",
    "def read_file(file):\n",
    "    arr = []\n",
    "    with open(file, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for lines in reader:\n",
    "            if (len(lines) == 2): arr.append(float(lines[1]))\n",
    "            else: \n",
    "                try:\n",
    "                    arr.append(list(np.float_(lines[1:])))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                    \n",
    "        \n",
    "        if arr[0] == 0: arr.pop(0)\n",
    "        \n",
    "        # for a in arr:\n",
    "        #     list(np.float_(a))\n",
    "            # for num in a:\n",
    "                # list(np.float_(list_name)\n",
    "                # num = float(num)\n",
    "\n",
    "    return arr\n",
    "\n",
    "pdiffs_abs = read_file(pdiffs_abs_file)\n",
    "pdiffs_rel = read_file(pdiffs_rel_file)\n",
    "features_abs = read_file(f_abs_file)\n",
    "features_rel = read_file(f_rel_file)\n",
    "\n",
    "# features_rel.pop(0) # since first entry is just 0, 1, 2, 3, 4,5 \n",
    "\n",
    "print(pdiffs_abs)\n",
    "print(pdiffs_rel)\n",
    "print(features_abs)\n",
    "# print(len(feature_abs))\n",
    "print(features_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through the optimized features --> predict the corresponding y val --> convert to phi (relevance)\n",
    "# this abs(relevance - actual relevance) is returned\n",
    "def calculate_relevance_difference(params, pdiffs): \n",
    "    difference_of_relevance = []\n",
    "\n",
    "    for i in range(len(pdiffs)):\n",
    "        x_param = [params[i]]\n",
    "                \n",
    "        # predict y from the optimized test case --> calculate phi from it\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        y_pred_rel = phi.phi(pd.Series(y_pred[0]), phi_parms=ph)\n",
    "        \n",
    "        # true y --> calculate phi\n",
    "        y_true_rel = phis[i * 10]\n",
    "        # y_true = y_test.iloc[i * 10][predictor]\n",
    "        # y_true_rel = phi.phi(pd.Series(y_true), phi_parms=ph)\n",
    "        \n",
    "        relevance_difference = abs(y_pred_rel - y_true_rel)\n",
    "        difference_of_relevance.append(relevance_difference[0])\n",
    "    \n",
    "    return difference_of_relevance\n",
    "\n",
    "\n",
    "# THIS IS FOR THE BO RELEVANCE BECAUSE WE OPTIMIZED THE MODEL TO FIND RELEVANCE DIFFERENCE\n",
    "# THE PDIFF RETURNED IS REL DIFF --> MUST CALCULATE OPTIMIZED Y INSTEAD OF REL\n",
    "\n",
    "# through the optimized features --> predict corresponding y val -->\n",
    "# find absolute difference between this predicted value and the actual y val \n",
    "def calculate_y_difference(params, pdiffs):\n",
    "    difference_of_y = []\n",
    "    \n",
    "    for i in range(len(pdiffs)):\n",
    "        x_param = [params[i]]\n",
    "\n",
    "        # predict y from the optimized test case BEST CASE FROM PREDICTION\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        y_orig = y_test.iloc[i * 10][predictor]\n",
    "        \n",
    "        abs_diff = abs(y_pred - y_orig)\n",
    "        difference_of_y.append(abs_diff[0])\n",
    "    \n",
    "    return difference_of_y\n",
    "\n",
    "\n",
    "difference_of_relevance_abs_y = calculate_relevance_difference(features_abs, pdiffs_abs)\n",
    "difference_of_relevance_abs_rel_y = calculate_relevance_difference(features_rel, pdiffs_rel)\n",
    "\n",
    "print(difference_of_relevance_abs_y)\n",
    "print(difference_of_relevance_abs_rel_y)\n",
    "\n",
    "the_difference_of_y = calculate_y_difference(features_rel, pdiffs_rel)\n",
    "\n",
    "print(the_difference_of_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot 1\n",
    "# pdiffs_abs_y\n",
    "# pdiffs_abs_rel\n",
    "\n",
    "\n",
    "# plot_data = [pdiffs_abs_y, pdiffs_abs_rel]\n",
    "plot_data = [pdiffs_abs, the_difference_of_y]\n",
    "\n",
    " \n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Creating axes instance\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.set_xticklabels(['BO Y Difference', 'BO Y Relevance Difference'])\n",
    "ax.set_ylabel('Y true - Y predicted')\n",
    " \n",
    "# Creating plot\n",
    "bp = ax.boxplot(plot_data)\n",
    " \n",
    "# show plot\n",
    "plt.show()\n",
    "\n",
    "# plot 2\n",
    "# phi_diff_rel = the relevance difference between the actual y AND the precited y (from optimized test case) OF MODEL 1\n",
    "plot_data1 = [difference_of_relevance_abs_y, difference_of_relevance_abs_rel_y]\n",
    " \n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Creating axes instance\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.set_xticklabels(['BO Y Difference', 'BO Y Relevance Difference'])\n",
    "ax.set_ylabel('Y true - Y predicted (Relevance)')\n",
    " \n",
    "# Creating plot\n",
    "bp = ax.boxplot(plot_data1)\n",
    " \n",
    "# show plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot_data2 = [difference_of_relevance_abs_y, pdiffs_rel]\n",
    " \n",
    "# fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# # Creating axes instance\n",
    "# ax = fig.add_axes([0, 0, 1, 1])\n",
    "# ax.set_xticklabels(['BO Y Difference', 'BO Y Relevance Difference'])\n",
    "# ax.set_ylabel('Y true - Y predicted (Relevance)')\n",
    " \n",
    "# # Creating plot\n",
    "# bp = ax.boxplot(plot_data2)\n",
    " \n",
    "# # show plot\n",
    "# plt.show()\n",
    "\n",
    "# bayes opt reducing min\n",
    "# bayes opt reducing rel between predicted and true\n",
    "\n",
    "# BO with relevance is better because median is lower --> closer distance, lower cost\n",
    "# ends probbly steep case where small change create large difference\n",
    "# slightly greate rrelvance difference for rel BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "# find distance between actual and predicted params (features)\n",
    "\n",
    "params_orig = []\n",
    "X_f = X_test.values.tolist()\n",
    "print(X_f)\n",
    "for i in range(0, len(X_test), 10):\n",
    "    params_orig.append(X_f[i])\n",
    "\n",
    "# print(X_test)\n",
    "print(params_orig)\n",
    "print(len(params_orig))\n",
    "\n",
    "\n",
    "\n",
    "f1 = np.array(features_abs)\n",
    "f2 = np.array(features_rel)\n",
    "\n",
    "distance_abs = euclidean_distances(params_orig, f1)\n",
    "distance_rel = euclidean_distances(params_orig, f2)\n",
    "\n",
    "d_abs = []\n",
    "d_rel = []\n",
    "\n",
    "for d in distance_abs:\n",
    "    if np.mean(d) > 1000: continue\n",
    "    d_abs.append(np.mean(d))\n",
    "    \n",
    "for d in distance_rel:\n",
    "    if np.mean(d) > 1000: continue\n",
    "    d_rel.append(np.mean(d))\n",
    "    \n",
    "print(d_abs)\n",
    "print(d_rel)\n",
    "\n",
    "relative_diff = []\n",
    "for i in range(len(d_rel)):\n",
    "    relative_diff.append(((d_rel[i] - d_abs[i]) / d_abs[i]) * 100)\n",
    "\n",
    "# relative_diff = (d_rel - d_abs) / d_abs\n",
    "print(relative_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [relative_diff]\n",
    " \n",
    "fig = plt.figure(figsize =(10, 7))\n",
    " \n",
    "# Creating axes instance\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.set_xticklabels(['Relative Percentual Difference'])\n",
    "ax.set_ylabel('Algorithmic Recourse (Y - Yt)')\n",
    " \n",
    "# Creating plot\n",
    "bp = ax.boxplot(data)\n",
    " \n",
    "# show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
